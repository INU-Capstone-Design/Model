## 1. 위키피디아 크롤링을 통해 데이터 수집
####  데이터 추가 수집

## 2. 수집된 데이터로 word2vec 모델을 활용하여 임베딩 된 단어 벡터 추출
####  데이터를 더 키워야 함

## 3. word2word로 추출된 데이터 번역

#### nltk의 토큰화 말고 keras의 text_to_word_sequence로 토큰화 사용 시 마침표, 컴마, 느낌표 등 구두점 제거 해주고, 어퍼스트로피 보존
####  토큰화를 keras로도 해볼 예정 
